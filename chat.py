import config
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import os


# --- 1. í›ˆë ¨ ë•Œì™€ 'ì™„ì „íˆ ë™ì¼í•œ' 4ë¹„íŠ¸ ì„¤ì • ---
# (bfloat16 ëŒ€ì‹  float16ì„ ì‚¬ìš©í–ˆë˜ ê²ƒì´ í•µì‹¬)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# --- 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (Llama 3.1) ---
print(f"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘: {config.BASE_MODEL_ID}")
base_model = AutoModelForCausalLM.from_pretrained(
    config.BASE_MODEL_ID,
    quantization_config=quantization_config,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token

# --- 3. [â­ï¸í•µì‹¬] 'Growit ì–´ëŒ‘í„°' ë®ì–´ì”Œìš°ê¸° ---
# 'final_checkpoint' ê²½ë¡œë¥¼ configì—ì„œ ê°€ì ¸ì˜´
adapter_path = os.path.join(config.FINETUNED_MODEL_PATH, "final_checkpoint")

print(f"'{adapter_path}'ì—ì„œ Growit ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")

# PeftModelì„ ì‚¬ìš©í•´ ë² ì´ìŠ¤ ëª¨ë¸ ìœ„ì— í›ˆë ¨ëœ ì–´ëŒ‘í„°ë¥¼ ë®ì–´ì”Œì›€
model = PeftModel.from_pretrained(base_model, adapter_path)

# 4ë¹„íŠ¸ ëª¨ë¸ + Peft ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨ì´ ì•„ë‹Œ 'ì¶”ë¡ (evaluation)' ëª¨ë“œë¡œ ì„¤ì •
model = model.eval()

print("--- ğŸ¤– Growit AI (Finetuned) ì¤€ë¹„ ì™„ë£Œ ---")
print("('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ)")

# --- 4. ì±„íŒ… ë£¨í”„ ---
history = [] # ê°„ë‹¨í•œ ëŒ€í™” ê¸°ë¡
system_message = {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¼ê¸°ì— ê³µê°í•˜ë©° ëŒ€í™”í•˜ëŠ” ì¹œêµ¬ 'Growit'ì…ë‹ˆë‹¤."}

while True:
    try:
        prompt = input("User: ")
        if prompt.lower() == "exit":
            break

        # 'messages' í˜•ì‹ êµ¬ì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì´ì „ ëŒ€í™” + í˜„ì¬ ì…ë ¥)
        messages = [system_message] + history + [{"role": "user", "content": prompt}]
        
        input_ids = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)
        
        # --- 5. ëª¨ë¸ ë‹µë³€ ìƒì„± (ì´ ë¶€ë¶„ì´ 10ë¶„ ê±¸ë¦¼) ---
        outputs = model.generate(
            input_ids,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id # pad_token ì„¤ì •
        )
        
        response_ids = outputs[0][input_ids.shape[-1]:]
        result_text = tokenizer.decode(response_ids, skip_special_tokens=True)
        
        print(f"\nGrowit AI: {result_text}")
        
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        history.append({"role": "user", "content": prompt})
        history.append({"role": "assistant", "content": result_text})

    except KeyboardInterrupt:
        print("\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break